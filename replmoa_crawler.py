import csv
import json
import os
import random
import time
import io
import hashlib
import signal
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import psycopg2
from psycopg2.extras import RealDictCursor

# ============================================
# t3.small (2GB) ì†ë„ ìµœì í™” ì„¤ì •
# ============================================
SPEED_MODE = os.environ.get("CRAWL_SPEED_MODE", "normal")  # "fast" ë˜ëŠ” "normal"

if SPEED_MODE == "fast":
    MAX_WORKERS = 10         # ë™ì‹œ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (2GBì—ì„œ ì•ˆì „í•œ ìµœëŒ€ì¹˜)
    BATCH_SIZE = 40          # ë°°ì¹˜ í¬ê¸°
    SLEEP_BETWEEN_BATCH = 0.15  # ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„
    SLEEP_BETWEEN_REQUEST = 0.08  # ìš”ì²­ ê°„ ìµœì†Œ ëŒ€ê¸°
    URL_COLLECT_WORKERS = 5   # URL ìˆ˜ì§‘ ë™ì‹œ ìš”ì²­ ìˆ˜
else:
    MAX_WORKERS = 7          # ë™ì‹œ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
    BATCH_SIZE = 30          # ë°°ì¹˜ í¬ê¸°
    SLEEP_BETWEEN_BATCH = 0.3  # ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„
    SLEEP_BETWEEN_REQUEST = 0.15  # ìš”ì²­ ê°„ ìµœì†Œ ëŒ€ê¸°
    URL_COLLECT_WORKERS = 4   # URL ìˆ˜ì§‘ ë™ì‹œ ìš”ì²­ ìˆ˜
# ============================================
SKIP_S3_UPLOAD = os.environ.get("CRAWL_SKIP_S3", "false").lower() == "true"
# ============================================

# ì¤‘ì§€ í”Œë˜ê·¸ í™•ì¸
STOP_FLAG_PATH = os.environ.get("CRAWL_STOP_FLAG", "")
STOP_REQUESTED = False

def check_stop_flag():
    """ì¤‘ì§€ ìš”ì²­ í™•ì¸"""
    global STOP_REQUESTED
    if STOP_REQUESTED:
        return True
    if STOP_FLAG_PATH and os.path.exists(STOP_FLAG_PATH):
        STOP_REQUESTED = True
        print("\n[STOP] ì¤‘ì§€ ìš”ì²­ ê°ì§€ë¨! í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
        return True
    return False

def signal_handler(signum, frame):
    """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ (SIGTERM, SIGINT)"""
    global STOP_REQUESTED
    STOP_REQUESTED = True
    print(f"\n[STOP] ì‹œê·¸ë„ {signum} ìˆ˜ì‹ ë¨. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    sys.exit(0)

# ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# AWS S3 ì„¤ì •
try:
    import boto3
    from botocore.exceptions import ClientError
    S3_ENABLED = True
except ImportError:
    S3_ENABLED = False
    print("[WARNING] boto3ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install boto3ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

# AWS S3 ì„¤ì •
AWS_REGION = os.environ.get("AWS_REGION", "ap-northeast-2")
AWS_S3_BUCKET = os.environ.get("AWS_S3_BUCKET", "wiznoble-image")
AWS_ACCESS_KEY_ID = os.environ.get("AWS_ACCESS_KEY_ID", "")
AWS_SECRET_ACCESS_KEY = os.environ.get("AWS_SECRET_ACCESS_KEY", "")

# S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
s3_client = None
if S3_ENABLED and AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:
    try:
        s3_client = boto3.client(
            's3',
            region_name=AWS_REGION,
            aws_access_key_id=AWS_ACCESS_KEY_ID,
            aws_secret_access_key=AWS_SECRET_ACCESS_KEY
        )
        print(f"[S3] AWS S3 ì—°ê²° ì„±ê³µ: {AWS_S3_BUCKET}")
    except Exception as e:
        print(f"[S3 ERROR] S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        s3_client = None


def upload_image_to_s3(image_url: str, prefix: str = "crawled") -> Optional[str]:
    """
    ì™¸ë¶€ ì´ë¯¸ì§€ URLì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ S3ì— ì—…ë¡œë“œ
    Returns: S3 URL ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not s3_client:
        return image_url  # S3 ì‚¬ìš© ë¶ˆê°€ ì‹œ ì›ë³¸ URL ë°˜í™˜
    
    if not image_url or not image_url.startswith("http"):
        return image_url
    
    try:
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        response = http_session.get(image_url, timeout=30)
        if response.status_code != 200:
            print(f"[S3] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url}")
            return image_url
        
        # íŒŒì¼ í™•ì¥ì ê²°ì •
        content_type = response.headers.get('Content-Type', 'image/jpeg')
        ext_map = {
            'image/jpeg': '.jpg',
            'image/jpg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp',
        }
        ext = ext_map.get(content_type, '.jpg')
        
        # URLì—ì„œ í™•ì¥ì ì¶”ì¶œ ì‹œë„
        parsed = urlparse(image_url)
        path_ext = os.path.splitext(parsed.path)[1].lower()
        if path_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            ext = path_ext if path_ext != '.jpeg' else '.jpg'
        
        # ê³ ìœ  íŒŒì¼ëª… ìƒì„± (URL í•´ì‹œ + íƒ€ì„ìŠ¤íƒ¬í”„)
        url_hash = hashlib.md5(image_url.encode()).hexdigest()[:12]
        timestamp = int(time.time() * 1000)
        s3_key = f"{prefix}/{timestamp}_{url_hash}{ext}"
        
        # S3ì— ì—…ë¡œë“œ
        s3_client.put_object(
            Bucket=AWS_S3_BUCKET,
            Key=s3_key,
            Body=response.content,
            ContentType=content_type,
        )
        
        # S3 URL ë°˜í™˜
        s3_url = f"https://{AWS_S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{s3_key}"
        return s3_url
    
    except Exception as e:
        print(f"[S3 ERROR] ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨ ({image_url}): {e}")
        return image_url  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ë°˜í™˜


def upload_images_batch_to_s3(image_urls: List[str], prefix: str = "crawled") -> List[str]:
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë³‘ë ¬ë¡œ S3ì— ì—…ë¡œë“œ (ìˆœì„œ ë³´ì¥)
    Returns: S3 URL ë¦¬ìŠ¤íŠ¸ (ì›ë˜ ìˆœì„œ ìœ ì§€)
    """
    if not s3_client or not image_urls:
        return image_urls
    
    # ìˆœì„œ ë³´ì¥ì„ ìœ„í•´ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì²˜ë¦¬
    s3_urls = [None] * len(image_urls)
    
    with ThreadPoolExecutor(max_workers=3) as executor:  # S3 ì—…ë¡œë“œëŠ” 3ê°œë¡œ ì œí•œ
        # (index, url) íŠœí”Œë¡œ ì œì¶œí•˜ì—¬ ìˆœì„œ ì¶”ì 
        futures = {
            executor.submit(upload_image_to_s3, url, prefix): (idx, url) 
            for idx, url in enumerate(image_urls)
        }
        for future in as_completed(futures):
            idx, original_url = futures[future]
            try:
                s3_url = future.result()
                s3_urls[idx] = s3_url  # ì›ë˜ ìœ„ì¹˜ì— ì €ì¥
            except Exception as e:
                print(f"[S3 ERROR] ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                s3_urls[idx] = original_url  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ìœ ì§€
    
    return s3_urls


# ì„¤ì •
SITEMAP_URL = "https://replmoa1.com/sitemap3.xml"
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/91.0.4472.124 Safari/537.36"
    )
}

# HTTP Session (ì—°ê²° ì¬ì‚¬ìš© â†’ TCP handshake ì ˆì•½, ì†ë„ 2~3ë°° í–¥ìƒ)
http_session = requests.Session()
http_session.headers.update(HEADERS)
# ì—°ê²° í’€ í¬ê¸°ë¥¼ ì›Œì»¤ ìˆ˜ì— ë§ì¶° ì„¤ì •
adapter = requests.adapters.HTTPAdapter(
    pool_connections=MAX_WORKERS + URL_COLLECT_WORKERS,
    pool_maxsize=MAX_WORKERS + URL_COLLECT_WORKERS,
    max_retries=2
)
http_session.mount('https://', adapter)
http_session.mount('http://', adapter)
CSV_FILENAME = "replmoa_products.csv"
DB_CONFIG = {
    "host": os.environ.get("DB_HOST", "localhost"),
    "port": int(os.environ.get("DB_PORT", "5432")),
    "dbname": os.environ.get("DB_NAME", "modern_shop"),
    "user": os.environ.get("DB_USER", "postgres"),
    "password": os.environ.get("DB_PASSWORD", "1234"),
}
_raw_limit = os.environ.get("CRAWL_LIMIT", "500")
MAX_SAVE = 999999 if _raw_limit == "0" else int(_raw_limit)  # 0 = ë¬´ì œí•œ (ì „ì²´ í¬ë¡¤ë§)
CATEGORY_FILTER = os.environ.get("CRAWL_CATEGORY", "")  # ì˜ˆ: "ë‚¨ì„±", "ì—¬ì„±", "ë‚¨ì„± > ì§€ê°‘" ë“±
URL_SOURCE = os.environ.get("CRAWL_URL_SOURCE", "both")  # "sitemap", "category", "both"
MAX_DB_PRICE = 9999999999999.99  # numeric(15,2) í™•ì¥ í›„ ìƒí•œ (ì•½ 10ì¡°ì›)

# ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ gc import
import gc


def slugify(txt: str) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ URL-safe ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜"""
    import re
    slug = (
        re.sub(r"[^0-9a-zA-Zê°€-í£\-]+", "-", txt.strip().lower())
        .strip("-")
        .replace("--", "-")
    )
    return slug or "etc"


def normalize_category_4depth(cat_raw: str) -> Dict[str, any]:
    """
    'ë‚¨ì„± > ê°€ë°© > ê³ ì•¼ë“œ > í¬ë¡œìŠ¤&ìˆ„ë”ë°±' í˜•íƒœë¥¼ 4ëìŠ¤ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¡œ ë³€í™˜
    3ëìŠ¤('ë‚¨ì„± > ì§€ê°‘ > í”„ë¼ë‹¤')ë„ í˜¸í™˜ ì²˜ë¦¬
    """
    parts = [p.strip() for p in cat_raw.split(">") if p.strip()]
    
    # ëŒ€ë¶„ë¥˜ ë§¤í•‘
    main_map = {
        "ë‚¨ì„±": "men",
        "ì—¬ì„±": "women",
        "êµ­ë‚´ì¶œê³ ìƒí’ˆ": "domestic",
        "êµ­ë‚´ì¶œê³  ìƒí’ˆ": "domestic",
        "êµ­ë‚´ì¶œê³ ": "domestic",
        "êµ­ë‚´ ì¶œê³ ": "domestic",
    }
    
    # ì¤‘ë¶„ë¥˜ ë§¤í•‘ (ìƒí’ˆ ì¢…ë¥˜)
    sub_map = {
        "ê°€ë°©": "bag",
        "ì§€ê°‘": "wallet",
        "ì‹œê³„": "watch",
        "ì‹ ë°œ": "shoes",
        "ë²¨íŠ¸": "belt",
        "ì•…ì„¸ì„œë¦¬": "accessory",
        "ì•¡ì„¸ì„œë¦¬": "accessory",
        "ëª¨ì": "hat",
        "ì˜ë¥˜": "clothing",
        "ì„ ê¸€ë¼ìŠ¤&ì•ˆê²½": "glasses",
        "ì„ ê¸€ë¼ìŠ¤": "glasses",
        "ì•ˆê²½": "glasses",
        "ê¸°íƒ€": "etc",
        "ê°€ë°©&ì§€ê°‘": "bag-wallet",
        "íŒ¨ì…˜ì¡í™”": "fashion",
        "ìƒí™œ&ì£¼ë°©ìš©í’ˆ": "home",
        "í–¥ìˆ˜": "perfume",
        "ë¼ì´í„°": "lighter",
    }
    
    # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (depth4 - ê°€ë°©/ì§€ê°‘/ì‹ ë°œ/ì˜ë¥˜ í•˜ìœ„ ë“±)
    detail_map = {
        # ê°€ë°© í•˜ìœ„
        "í¬ë¡œìŠ¤&ìˆ„ë”ë°±": "crossbody-shoulder",
        "í¬ë¡œìŠ¤ë°±": "crossbody",
        "ìˆ„ë”ë°±": "shoulder",
        "í† íŠ¸ë°±": "tote",
        "í´ëŸ¬ì¹˜": "clutch",
        "í´ëŸ¬ì¹˜&íŒŒìš°ì¹˜": "clutch-pouch",
        "íŒŒìš°ì¹˜": "pouch",
        "ë°±íŒ©": "backpack",
        "ì„œë¥˜ê°€ë°©": "briefcase",
        "ì—¬í–‰ê°€ë°©": "luggage",
        "ë¯¸ë‹ˆë°±": "mini",
        "í•¸ë“œë°±": "handbag",
        "í˜¸ë³´ë°±": "hobo",
        "ë²„í‚·ë°±": "bucket",
        # ì§€ê°‘ í•˜ìœ„
        "ì¹´ë“œì§€ê°‘": "card-wallet",
        "ë°˜ì§€ê°‘": "half-wallet",
        "ì¥ì§€ê°‘": "long-wallet",
        "ì§€í¼ì›”ë ›": "zip-wallet",
        "ì½”ì¸ì§€ê°‘": "coin-wallet",
        "í‚¤í™€ë”": "key-holder",
        # ì‹ ë°œ í•˜ìœ„
        "ìŠ¤ë‹ˆì»¤ì¦ˆ": "sneakers",
        "ë¡œí¼": "loafer",
        "ë¶€ì¸ ": "boots",
        "ìƒŒë“¤": "sandal",
        "ìŠ¬ë¦¬í¼": "slipper",
        "í": "heel",
        "í”Œë«": "flat",
        "ë®¬": "mule",
        # ì˜ë¥˜ í•˜ìœ„
        "ì•„ìš°í„°": "outer",
        "ìì¼“": "jacket",
        "ì½”íŠ¸": "coat",
        "íŒ¨ë”©": "padding",
        "ë‹¤ìš´": "down",
        "ì í¼": "jumper",
        "ìƒì˜": "top",
        "í‹°ì…”ì¸ ": "tshirt",
        "ë°˜íŒ”": "short-sleeve",
        "ê¸´íŒ”": "long-sleeve",
        "ë‹ˆíŠ¸": "knit",
        "ë§¨íˆ¬ë§¨": "sweatshirt",
        "í›„ë“œ": "hoodie",
        "ì…”ì¸ ": "shirt",
        "ë¸”ë¼ìš°ìŠ¤": "blouse",
        "í•˜ì˜": "bottom",
        "íŒ¬ì¸ ": "pants",
        "ë°”ì§€": "pants",
        "ì²­ë°”ì§€": "jeans",
        "ë°ë‹˜": "denim",
        "ìŠ¤ì»¤íŠ¸": "skirt",
        "ì¹˜ë§ˆ": "skirt",
        "ë°˜ë°”ì§€": "shorts",
        "ì›í”¼ìŠ¤": "dress",
        "ë“œë ˆìŠ¤": "dress",
        "ì •ì¥": "suit",
        "íŠ¸ë ˆì´ë‹": "training",
        "ì„¸íŠ¸": "set",
        # ì•…ì„¸ì„œë¦¬ í•˜ìœ„
        "ëª©ê±¸ì´": "necklace",
        "íŒ”ì°Œ": "bracelet",
        "ë°˜ì§€": "ring",
        "ê·€ê±¸ì´": "earring",
        "ë¸Œë¡œì¹˜": "brooch",
        "ìŠ¤ì¹´í”„": "scarf",
        "ë¨¸í”ŒëŸ¬": "muffler",
        "ë„¥íƒ€ì´": "necktie",
        "ì¥ê°‘": "gloves",
        "ì–‘ë§": "socks",
    }
    
    result = {
        "depth1": None,
        "depth2": None,
        "depth3": None,
        "depth4": None,
        "full_name": cat_raw,
        "leaf_slug": None,
    }
    
    # ëŒ€ë¶„ë¥˜ (depth1) - ì„±ë³„/ìœ í˜•: ë‚¨ì„±, ì—¬ì„±, êµ­ë‚´ì¶œê³ ìƒí’ˆ
    if parts:
        main_name = parts[0]
        main_slug = main_map.get(main_name, slugify(main_name))
        result["depth1"] = {"name": main_name, "slug": main_slug}
        result["leaf_slug"] = main_slug
    
    # ì¤‘ë¶„ë¥˜ (depth2) - ìƒí’ˆ ì¢…ë¥˜: ê°€ë°©, ì§€ê°‘, ì‹œê³„ ë“±
    if len(parts) > 1:
        sub_name = parts[1]
        sub_slug_base = sub_map.get(sub_name, slugify(sub_name))
        sub_slug = f"{result['depth1']['slug']}-{sub_slug_base}"
        result["depth2"] = {"name": sub_name, "slug": sub_slug, "parent_slug": result["depth1"]["slug"]}
        result["leaf_slug"] = sub_slug
    
    # ì†Œë¶„ë¥˜ (depth3) - ë¸Œëœë“œ: ê³ ì•¼ë“œ, í”„ë¼ë‹¤, êµ¬ì°Œ ë“±
    if len(parts) > 2:
        brand_name = parts[2]
        brand_slug = f"{result['depth2']['slug']}-{slugify(brand_name)}"
        result["depth3"] = {"name": brand_name, "slug": brand_slug, "parent_slug": result["depth2"]["slug"]}
        result["leaf_slug"] = brand_slug
    
    # ì„¸ë¶€ë¶„ë¥˜ (depth4) - ì„¸ë¶€ ì¹´í…Œê³ ë¦¬: í¬ë¡œìŠ¤&ìˆ„ë”ë°±, í† íŠ¸ë°± ë“±
    if len(parts) > 3:
        detail_name = parts[3]
        detail_slug_base = detail_map.get(detail_name, slugify(detail_name))
        detail_slug = f"{result['depth3']['slug']}-{detail_slug_base}"
        result["depth4"] = {"name": detail_name, "slug": detail_slug, "parent_slug": result["depth3"]["slug"]}
        result["leaf_slug"] = detail_slug
    
    return result


# ê¸°ì¡´ 3ëìŠ¤ í˜¸í™˜ ë˜í¼
def normalize_category_3depth(cat_raw: str) -> Dict[str, any]:
    """ê¸°ì¡´ í˜¸í™˜ìš© - normalize_category_4depthì˜ ë˜í¼"""
    return normalize_category_4depth(cat_raw)


def normalize_category(cat_raw: str) -> Dict[str, str]:
    """ê¸°ì¡´ í˜¸í™˜ìš© - ìµœì¢… ì¹´í…Œê³ ë¦¬ ì •ë³´ë§Œ ë°˜í™˜"""
    cat_info = normalize_category_4depth(cat_raw)
    
    name_parts = []
    for depth_key in ["depth1", "depth2", "depth3", "depth4"]:
        if cat_info[depth_key]:
            name_parts.append(cat_info[depth_key]["name"])
    
    name = " > ".join(name_parts) if name_parts else "ê¸°íƒ€"
    
    return {"name": name, "slug": cat_info["leaf_slug"] or "etc"}


def get_product_urls_from_sitemap() -> List[str]:
    """ì‚¬ì´íŠ¸ë§µì—ì„œ ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print(f"[SITEMAP] ì‚¬ì´íŠ¸ë§µ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {SITEMAP_URL}")
    try:
        response = http_session.get(SITEMAP_URL, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        urls: List[str] = []
        for loc in soup.find_all("loc"):
            url = loc.text
            if "item.php" in url:
                urls.append(url)

        print(f"[SITEMAP] ì‚¬ì´íŠ¸ë§µì—ì„œ {len(urls)}ê°œì˜ ìƒí’ˆ URL ë°œê²¬")
        return urls
    except Exception as exc:
        print(f"[SITEMAP] ì‚¬ì´íŠ¸ë§µ ë¡œë“œ ì‹¤íŒ¨: {exc}")
        return []


# ============================================
# ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ê¸°ë°˜ URL ìˆ˜ì§‘
# ============================================
BASE_URL = "https://replmoa1.com"

# ì•Œë ¤ì§„ ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ ID
KNOWN_TOP_CATEGORIES = {
    "10": "ë‚¨ì„±",
    "20": "ì—¬ì„±",
    "30": "êµ­ë‚´ì¶œê³ ìƒí’ˆ",
}



def get_product_urls_from_category_page(ca_id: str, page: int = 1) -> List[str]:
    """
    ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ìƒí’ˆ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Returns: í•´ë‹¹ í˜ì´ì§€ì˜ ìƒí’ˆ URL ë¦¬ìŠ¤íŠ¸ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€)
    """
    import re
    url = f"{BASE_URL}/shop/list.php?ca_id={ca_id}&page={page}"
    try:
        time.sleep(SLEEP_BETWEEN_REQUEST)
        response = http_session.get(url, timeout=15)
        if response.status_code != 200:
            return []
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ìƒí’ˆ ë§í¬ ì¶”ì¶œ (item.php?it_id=xxxxx)
        # "ë² ìŠ¤íŠ¸ìƒí’ˆ" ì˜ì—­ ì œì™¸ - ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ ìƒí’ˆë§Œ ì¶”ì¶œ
        product_urls = []
        seen = set()
        
        # ë² ìŠ¤íŠ¸ìƒí’ˆ it_idë¥¼ ë¨¼ì € ìˆ˜ì§‘í•˜ì—¬ ì œì™¸
        best_ids = set()
        best_section = soup.select_one(".best_item, .best_products, #best")
        if best_section:
            for a_tag in best_section.find_all("a", href=True):
                m = re.search(r"it_id=(\d+)", a_tag["href"])
                if m:
                    best_ids.add(m.group(1))
        
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            if "item.php" in href and "it_id=" in href:
                match = re.search(r"it_id=(\d+)", href)
                if match:
                    it_id = match.group(1)
                    if it_id not in seen:
                        seen.add(it_id)
                        clean_url = f"{BASE_URL}/shop/item.php?it_id={it_id}"
                        product_urls.append(clean_url)
        
        return product_urls
        
    except Exception as e:
        print(f"[CATEGORY] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ({ca_id}, page={page}): {e}")
        return []


def _fetch_category_page_batch(ca_id: str, pages: List[int]) -> Dict[int, List[str]]:
    """ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    results = {}
    
    def fetch_one(page):
        return page, get_product_urls_from_category_page(ca_id, page)
    
    with ThreadPoolExecutor(max_workers=URL_COLLECT_WORKERS) as executor:
        futures = {executor.submit(fetch_one, p): p for p in pages}
        for future in as_completed(futures):
            try:
                page, urls = future.result()
                results[page] = urls
            except Exception:
                results[futures[future]] = []
    
    return results


def get_product_urls_from_categories(category_filter: str = "") -> List[str]:
    """
    ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬(ë‚¨ì„±/ì—¬ì„±/êµ­ë‚´ì¶œê³ ) ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ëê¹Œì§€ ìˆœíšŒí•˜ë©° 
    ëª¨ë“  ìƒí’ˆ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    ë³‘ë ¬ í˜ì´ì§€ ìˆ˜ì§‘: í•œ ë²ˆì— ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ë™ì‹œì— ìš”ì²­í•˜ì—¬ ìˆ˜ì§‘ ì†ë„ë¥¼ ëŒ€í­ í–¥ìƒí•©ë‹ˆë‹¤.
    """
    import re
    print("[CATEGORY] ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ìƒí’ˆ URL ìˆ˜ì§‘ ì‹œì‘...")
    print(f"[CATEGORY] ë³‘ë ¬ ìˆ˜ì§‘ ëª¨ë“œ (ë™ì‹œ {URL_COLLECT_WORKERS}í˜ì´ì§€ì”©)")
    
    # í¬ë¡¤ë§ ëŒ€ìƒ ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ ê²°ì •
    target_categories = dict(KNOWN_TOP_CATEGORIES)
    
    if category_filter:
        filter_lower = category_filter.lower().strip().split(">")[0].strip()
        filtered = {}
        for ca_id, name in KNOWN_TOP_CATEGORIES.items():
            if filter_lower in name.lower():
                filtered[ca_id] = name
        if filtered:
            target_categories = filtered
            print(f"[CATEGORY] í•„í„° '{category_filter}' ì ìš©: {list(target_categories.values())}")
    
    all_urls = []
    seen_urls = set()
    
    for ca_id, cat_name in target_categories.items():
        if check_stop_flag():
            break
        
        print(f"[CATEGORY] === '{cat_name}' (ca_id={ca_id}) ë³‘ë ¬ í˜ì´ì§€ ìˆœíšŒ ì‹œì‘ ===")
        page = 1
        max_pages = 2000
        cat_urls = 0
        consecutive_no_new = 0
        finished = False
        
        while page <= max_pages and not finished:
            if check_stop_flag():
                break
            
            # í•œ ë²ˆì— URL_COLLECT_WORKERS í˜ì´ì§€ì”© ë³‘ë ¬ ìˆ˜ì§‘
            page_batch = list(range(page, min(page + URL_COLLECT_WORKERS, max_pages + 1)))
            batch_results = _fetch_category_page_batch(ca_id, page_batch)
            
            # ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (í˜ì´ì§€ ë²ˆí˜¸ ìˆœ)
            for p in sorted(batch_results.keys()):
                if check_stop_flag():
                    finished = True
                    break
                
                urls = batch_results[p]
                
                # ìƒí’ˆì´ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€
                if not urls:
                    print(f"[CATEGORY] '{cat_name}' page={p}: ìƒí’ˆ ì—†ìŒ â†’ ì™„ë£Œ")
                    finished = True
                    break
                
                # í˜„ì¬ í˜ì´ì§€ì˜ it_id ì¶”ì¶œ
                current_ids = set()
                for u in urls:
                    m = re.search(r"it_id=(\d+)", u)
                    if m:
                        current_ids.add(m.group(1))
                
                new_count = 0
                for url in urls:
                    if url not in seen_urls:
                        seen_urls.add(url)
                        all_urls.append(url)
                        new_count += 1
                        cat_urls += 1
                
                # ìƒˆ URLì´ ì—†ìœ¼ë©´ ì¹´ìš´íŠ¸
                if new_count == 0:
                    consecutive_no_new += 1
                    if consecutive_no_new >= 3:
                        print(f"[CATEGORY] '{cat_name}' page={p}: 3í˜ì´ì§€ ì—°ì† ìƒˆ URL ì—†ìŒ â†’ ì™„ë£Œ")
                        finished = True
                        break
                else:
                    consecutive_no_new = 0
            
            # ì§„í–‰ ìƒí™© ë¡œê·¸
            if page % 50 < URL_COLLECT_WORKERS:
                print(f"[CATEGORY] '{cat_name}' page~{page + len(page_batch) - 1}: ëˆ„ì  {cat_urls}ê°œ ìˆ˜ì§‘ ì¤‘...")
            
            page += len(page_batch)
        
        print(f"[CATEGORY] '{cat_name}': ì´ {cat_urls}ê°œ ìƒí’ˆ URL ìˆ˜ì§‘ ì™„ë£Œ (~{page-1}í˜ì´ì§€)")
    
    print(f"[CATEGORY] ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì´ {len(all_urls)}ê°œ ìƒí’ˆ URL ìˆ˜ì§‘ ì™„ë£Œ")
    return all_urls


def get_product_urls() -> List[str]:
    """ì‚¬ì´íŠ¸ë§µê³¼ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ë³‘í•©í•˜ì—¬ ìƒí’ˆ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    all_urls = []
    seen = set()
    
    source = URL_SOURCE.lower().strip()
    
    # 1. ì‚¬ì´íŠ¸ë§µì—ì„œ ìˆ˜ì§‘
    if source in ("sitemap", "both"):
        sitemap_urls = get_product_urls_from_sitemap()
        for url in sitemap_urls:
            # URL ì •ê·œí™”
            clean = url.strip()
            if clean and clean not in seen:
                seen.add(clean)
                all_urls.append(clean)
        print(f"[COLLECT] ì‚¬ì´íŠ¸ë§µì—ì„œ {len(sitemap_urls)}ê°œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° í›„: {len(all_urls)}ê°œ)")
    
    # 2. ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘
    if source in ("category", "both"):
        category_urls = get_product_urls_from_categories(CATEGORY_FILTER)
        new_from_category = 0
        for url in category_urls:
            clean = url.strip()
            if clean and clean not in seen:
                seen.add(clean)
                all_urls.append(clean)
                new_from_category += 1
        print(f"[COLLECT] ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ {len(category_urls)}ê°œ ìˆ˜ì§‘ (ì‹ ê·œ: {new_from_category}ê°œ)")
    
    # 3. URL ì…”í”Œ (íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— í¸ì¤‘ë˜ì§€ ì•Šë„ë¡)
    random.shuffle(all_urls)
    
    print(f"[COLLECT] ì´ {len(all_urls)}ê°œ ê³ ìœ  ìƒí’ˆ URL ìˆ˜ì§‘ ì™„ë£Œ")
    return all_urls


def parse_product_options(soup: BeautifulSoup) -> List[Dict[str, any]]:
    """ìƒí’ˆ ì˜µì…˜(ì‚¬ì´ì¦ˆ, ì»¬ëŸ¬ ë“±)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    options = []
    
    option_selectors = [
        "#sit_opt_added select",
        ".sit_opt_added select",
        "#sit_option select",
        ".sit_option select",
        "select[name^='opt']",
        "select[id^='it_opt']",
        ".item_option select",
    ]
    
    for selector in option_selectors:
        for select_tag in soup.select(selector):
            option_name = ""
            label = select_tag.find_previous("label")
            if label:
                option_name = label.get_text(strip=True)
            else:
                prev = select_tag.find_previous(string=True)
                if prev:
                    option_name = prev.strip().rstrip(":")
            
            if not option_name:
                name_attr = select_tag.get("name", "") or select_tag.get("id", "")
                if "color" in name_attr.lower() or "ì»¬ëŸ¬" in name_attr:
                    option_name = "ì»¬ëŸ¬"
                elif "size" in name_attr.lower() or "ì‚¬ì´ì¦ˆ" in name_attr:
                    option_name = "ì‚¬ì´ì¦ˆ"
                else:
                    option_name = "ì˜µì…˜"
            
            option_values = []
            for opt in select_tag.find_all("option"):
                val = opt.get_text(strip=True)
                if val and "ì„ íƒ" not in val and val != "-":
                    import re
                    price_add = 0
                    if "(" in val and "ì›" in val:
                        price_match = re.search(r"\(([+-]?\s*[\d,]+)\s*ì›\)", val)
                        if price_match:
                            price_str = price_match.group(1).replace(",", "").replace(" ", "")
                            try:
                                price_add = int(price_str)
                            except:
                                pass
                        val = re.sub(r"\([+-]?\s*[\d,]+\s*ì›\)", "", val).strip()
                    
                    val = re.sub(r"\s*[+-]\s*\d+\s*ì›", "", val).strip()
                    
                    if val:
                        option_values.append({
                            "value": val,
                            "price_add": price_add
                        })
            
            if option_values:
                options.append({
                    "name": option_name,
                    "values": option_values
                })
    
    seen_names = set()
    unique_options = []
    for opt in options:
        if opt["name"] not in seen_names:
            seen_names.add(opt["name"])
            unique_options.append(opt)
    
    return unique_options


def parse_product_detail(url: str, upload_to_s3: bool = True) -> Optional[Dict[str, any]]:
    """ê°œë³„ ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        response = http_session.get(url, timeout=15)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        # 1. ìƒí’ˆëª… ì¶”ì¶œ
        title_tag = soup.select_one("#sit_title") or soup.select_one(".stitle")
        title = title_tag.text.strip() if title_tag else "ìƒí’ˆëª… ì—†ìŒ"

        # 2. ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        category = ""
        sit_ov = soup.select_one("#sit_ov")
        if sit_ov:
            text_candidates = [
                t.strip()
                for t in sit_ov.stripped_strings
                if ">" in t and "ìƒí’ˆê°„ëµì •ë³´" not in t
            ]
            if text_candidates:
                category = text_candidates[0]

        # 3. ì‹œì¤‘ê°€ê²© / íŒë§¤ê°€ê²©
        market_price = ""
        sale_price = ""
        market_tag = soup.select_one(".price_wr.price_og span")
        sale_tag = soup.select_one(".price_wr.price span")
        if market_tag:
            market_price = market_tag.get_text(strip=True)
        if sale_tag:
            sale_price = sale_tag.get_text(strip=True)

        # 4. ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        img_tag = soup.select_one("#sit_pvi_big img") or soup.select_one(".sit_pvi img")
        img_url = ""
        if img_tag:
            img_url = img_tag.get("src", "")
            if img_url and not img_url.startswith("http"):
                img_url = "https://replmoa1.com" + img_url

        # 5. ìƒì„¸ ì„¤ëª… ë‚´ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ
        desc_img_urls: List[str] = []
        seen = set()
        
        if img_url:
            seen.add(img_url)
            desc_img_urls.append(img_url)
        
        desc_selectors = [
            "#sit_inf_explan img",
            "#sit_inf img",
            ".sit_inf img", 
            "#sit_desc img",
            ".sit_desc img",
            ".item_explan img",
            ".product-detail img",
            ".view_content img",
            "#goods_spec img",
            ".goods_description img",
            ".detail_cont img",
            "[class*='detail'] img",
            "[class*='desc'] img",
            "[id*='detail'] img",
            ".product-content img",
            ".goods-view img",
            ".item-detail img",
            "#prdDetail img",
            ".detailArea img",
        ]
        
        for selector in desc_selectors:
            try:
                tags = soup.select(selector)
                for tag in tags:
                    src = tag.get("src") or tag.get("data-src") or tag.get("data-original") or tag.get("data-lazy") or ""
                    if not src:
                        continue
                    
                    abs_src = urljoin("https://replmoa1.com", src)
                    
                    if not any(ext in abs_src.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        continue
                    
                    width = tag.get("width", "")
                    height = tag.get("height", "")
                    try:
                        if width and int(width) < 50:
                            continue
                        if height and int(height) < 50:
                            continue
                    except:
                        pass
                    
                    if abs_src not in seen:
                        seen.add(abs_src)
                        desc_img_urls.append(abs_src)
            except Exception as e:
                continue

        # 6. ì˜µì…˜ ì¶”ì¶œ
        options = parse_product_options(soup)

        # 7. S3ì— ì´ë¯¸ì§€ ì—…ë¡œë“œ (SKIP_S3_UPLOAD=trueì´ë©´ ì›ë³¸ URL ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        if not SKIP_S3_UPLOAD and upload_to_s3 and s3_client:
            if img_url:
                s3_img_url = upload_image_to_s3(img_url, prefix="products")
                if s3_img_url and s3_img_url != img_url:
                    img_url = s3_img_url
            
            if desc_img_urls:
                s3_desc_urls = upload_images_batch_to_s3(desc_img_urls, prefix="products/desc")
                desc_img_urls = s3_desc_urls

        return {
            "ìƒí’ˆëª…": title,
            "ì¹´í…Œê³ ë¦¬": category,
            "ì‹œì¤‘ê°€ê²©": market_price,
            "íŒë§¤ê°€ê²©": sale_price,
            "ëŒ€í‘œì´ë¯¸ì§€": img_url,
            "ì„¤ëª…ì´ë¯¸ì§€ë“¤": ";".join(desc_img_urls),
            "URL": url,
            "ì˜µì…˜": options,
        }

    except Exception as exc:
        print(f"íŒŒì‹± ì—ëŸ¬ ({url}): {exc}")
        return None


def main() -> None:
    import threading
    from collections import deque

    speed_label = "âš¡ ê³ ì†" if SPEED_MODE == "fast" else "ì¼ë°˜"
    s3_label = "ìŠ¤í‚µ (ì›ë³¸ URL ì‚¬ìš©)" if SKIP_S3_UPLOAD else "í™œì„±í™”"
    print(f"[CONFIG] ëª¨ë“œ: {speed_label}, ì›Œì»¤: {MAX_WORKERS}, ë°°ì¹˜: {BATCH_SIZE}, ëŒ€ê¸°: {SLEEP_BETWEEN_BATCH}s")
    print(f"[CONFIG] S3 ì—…ë¡œë“œ: {s3_label}, URL ìˆ˜ì§‘ ë³‘ë ¬: {URL_COLLECT_WORKERS}í˜ì´ì§€")

    # ============================================
    # 1ë‹¨ê³„: ì‚¬ì´íŠ¸ë§µ URL ë¹ ë¥´ê²Œ ìˆ˜ì§‘ (ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘ìš©)
    # ============================================
    source = URL_SOURCE.lower().strip()
    seen_urls = set()
    urls = []

    if source in ("sitemap", "both"):
        sitemap_urls = get_product_urls_from_sitemap()
        for url in sitemap_urls:
            clean = url.strip()
            if clean and clean not in seen_urls:
                seen_urls.add(clean)
                urls.append(clean)
        print(f"[COLLECT] ì‚¬ì´íŠ¸ë§µì—ì„œ {len(urls)}ê°œ ìˆ˜ì§‘ â†’ ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘!")
    
    # ì¹´í…Œê³ ë¦¬ URLì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜ì§‘í•˜ì—¬ íì— ì¶”ê°€
    category_url_queue = deque()
    category_collect_done = threading.Event()
    
    def background_category_collect():
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¹´í…Œê³ ë¦¬ URLì„ ìˆ˜ì§‘í•˜ì—¬ íì— ë„£ëŠ” ìŠ¤ë ˆë“œ"""
        if source not in ("category", "both"):
            category_collect_done.set()
            return
        
        print("[CATEGORY-BG] ë°±ê·¸ë¼ìš´ë“œ ì¹´í…Œê³ ë¦¬ URL ìˆ˜ì§‘ ì‹œì‘...")
        cat_urls = get_product_urls_from_categories(CATEGORY_FILTER)
        new_count = 0
        for url in cat_urls:
            clean = url.strip()
            if clean and clean not in seen_urls:
                seen_urls.add(clean)
                category_url_queue.append(clean)
                new_count += 1
        print(f"[CATEGORY-BG] ì¹´í…Œê³ ë¦¬ì—ì„œ ì‹ ê·œ {new_count}ê°œ ì¶”ê°€ ì™„ë£Œ")
        category_collect_done.set()
    
    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
    cat_thread = threading.Thread(target=background_category_collect, daemon=True)
    cat_thread.start()

    if not urls and source == "category":
        # ì¹´í…Œê³ ë¦¬ ì „ìš© ëª¨ë“œ: ì²« ë°°ì¹˜ê°€ ì˜¬ ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
        print("[COLLECT] ì¹´í…Œê³ ë¦¬ URL ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘...")
        while len(category_url_queue) == 0 and not category_collect_done.is_set():
            time.sleep(1)
        urls = list(category_url_queue)
        category_url_queue.clear()
    
    if not urls and category_collect_done.is_set():
        print("ìƒí’ˆ URLì„ ì°¾ì§€ ëª»í•´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    random.shuffle(urls)
    print(f"í¬ë¡¤ë§ ì‹œì‘ (ì´ˆê¸° {len(urls)}ê°œ + ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ìˆ˜ì§‘ ì¤‘)...")

    # 2. DB ì—°ê²°
    if not DB_CONFIG["password"]:
        print("[ERROR] DB_PASSWORD í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    conn = psycopg2.connect(**DB_CONFIG)
    conn.autocommit = True
    cur = conn.cursor(cursor_factory=RealDictCursor)

    def slugify(text: str) -> str:
        import re
        slug = (
            re.sub(r"[^0-9a-zA-Z\-]+", "-", text.strip().lower())
            .strip("-")
            .replace("--", "-")
        )
        return slug or "etc"

    def ensure_category_single(name: str, slug: str, parent_id: int = None, parent_slug: str = None, depth: int = 1) -> int:
        cur.execute("SELECT id FROM categories WHERE slug=%s", (slug,))
        row = cur.fetchone()
        if row:
            if parent_slug:
                cur.execute(
                    "UPDATE categories SET parent_slug=%s WHERE slug=%s AND (parent_slug IS NULL OR parent_slug = '')",
                    (parent_slug, slug)
                )
            return row["id"]
        cur.execute(
            "INSERT INTO categories (name, slug, parent_id, parent_slug, depth, description) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id",
            (name, slug, parent_id, parent_slug, depth, "imported from crawler"),
        )
        return cur.fetchone()["id"]

    def ensure_category_4depth(cat_raw: str) -> int:
        cat_info = normalize_category_4depth(cat_raw)
        
        parent_id = None
        parent_slug = None
        final_id = None
        
        if cat_info["depth1"]:
            d1 = cat_info["depth1"]
            parent_id = ensure_category_single(d1["name"], d1["slug"], None, None, 1)
            parent_slug = d1["slug"]
            final_id = parent_id
        
        if cat_info["depth2"]:
            d2 = cat_info["depth2"]
            parent_id = ensure_category_single(d2["name"], d2["slug"], parent_id, parent_slug, 2)
            parent_slug = d2["slug"]
            final_id = parent_id
        
        if cat_info["depth3"]:
            d3 = cat_info["depth3"]
            parent_id = ensure_category_single(d3["name"], d3["slug"], parent_id, parent_slug, 3)
            parent_slug = d3["slug"]
            final_id = parent_id
        
        if cat_info["depth4"]:
            d4 = cat_info["depth4"]
            final_id = ensure_category_single(d4["name"], d4["slug"], parent_id, parent_slug, 4)
        
        return final_id or ensure_category_single("ê¸°íƒ€", "etc", None, None, 1)

    # ê¸°ì¡´ í˜¸í™˜ìš©
    def ensure_category_3depth(cat_raw: str) -> int:
        return ensure_category_4depth(cat_raw)

    def already_exists(name: str, category_id: int) -> bool:
        cur.execute(
            "SELECT id FROM products WHERE name=%s AND category_id=%s",
            (name, category_id),
        )
        return cur.fetchone() is not None

    # URL ê¸°ë°˜ ë¹ ë¥¸ ì¤‘ë³µ ì²´í¬ìš© ìºì‹œ (it_id â†’ True)
    # DBì— ì €ì¥ëœ ìƒí’ˆì˜ URLì—ì„œ it_idë¥¼ ì¶”ì¶œí•˜ì—¬ ìºì‹œ
    existing_it_ids = set()
    try:
        import re as _re
        cur.execute("SELECT description FROM products WHERE description LIKE '%it_id=%'")
        for row in cur:
            m = _re.search(r"it_id=(\d+)", row["description"])
            if m:
                existing_it_ids.add(m.group(1))
        print(f"[SKIP] ê¸°ì¡´ ìƒí’ˆ {len(existing_it_ids)}ê°œì˜ it_id ìºì‹œ ì™„ë£Œ")
    except Exception as e:
        print(f"[SKIP] it_id ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

    def is_already_crawled_by_url(url: str) -> bool:
        """URLì˜ it_idë¡œ ë¹ ë¥´ê²Œ ì¤‘ë³µ ì²´í¬ (DB ì¿¼ë¦¬ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ)"""
        import re as _re
        m = _re.search(r"it_id=(\d+)", url)
        if m and m.group(1) in existing_it_ids:
            return True
        return False

    def save_product_options(product_id: int, options: List[Dict]) -> int:
        option_count = 0
        for option in options:
            option_name = option.get("name", "ì˜µì…˜")
            for val_info in option.get("values", []):
                val = val_info.get("value", "")
                price_add = val_info.get("price_add", 0)
                if val:
                    cur.execute(
                        """
                        INSERT INTO product_options (product_id, option_name, option_value, price_adjustment, stock)
                        VALUES (%s, %s, %s, %s, %s)
                        ON CONFLICT DO NOTHING
                        """,
                        (product_id, option_name, val, price_add, 10),
                    )
                    option_count += 1
        return option_count

    def matches_category_filter(product_category: str) -> bool:
        if not CATEGORY_FILTER:
            return True
        
        product_cat_lower = product_category.lower().strip()
        filter_lower = CATEGORY_FILTER.lower().strip()
        
        if product_cat_lower.startswith(filter_lower):
            return True
        
        filter_parts = [p.strip() for p in filter_lower.split(">")]
        product_parts = [p.strip() for p in product_cat_lower.split(">")]
        
        if len(filter_parts) <= len(product_parts):
            match = all(
                filter_parts[i] == product_parts[i] 
                for i in range(len(filter_parts))
            )
            if match:
                return True
        
        return False
    
    if CATEGORY_FILTER:
        print(f"[FILTER] ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©: '{CATEGORY_FILTER}'")

    def to_price(val: str) -> float:
        digits = "".join([c for c in val if c.isdigit()])
        return float(digits) if digits else 0.0

    def fetch_and_filter(url_idx_tuple):
        idx, url = url_idx_tuple
        try:
            # ë¹ ë¥¸ ì¤‘ë³µ ì²´í¬ (íŒŒì‹± ì „ì— it_idë¡œ í™•ì¸ â†’ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì ˆì•½)
            if is_already_crawled_by_url(url):
                return None, idx, url, "ì´ë¯¸ ìˆ˜ì§‘ëœ ìƒí’ˆ (ìŠ¤í‚µ)"
            
            info = parse_product_detail(url)
            if not info:
                return None, idx, url, "íŒŒì‹± ì‹¤íŒ¨"
            
            product_category = info.get("ì¹´í…Œê³ ë¦¬") or "ê¸°íƒ€"
            if not matches_category_filter(product_category):
                return None, idx, url, f"ì¹´í…Œê³ ë¦¬ ë¶ˆì¼ì¹˜: {product_category}"
            
            return info, idx, url, None
        except Exception as e:
            return None, idx, url, str(e)

    count = 0
    scanned = 0
    retry_urls = []  # íƒ€ì„ì•„ì›ƒ/ì—ëŸ¬ ë°œìƒí•œ URL (ë‚˜ì¤‘ì— ì¬ì‹œë„)
    
    def save_product_to_db(info):
        nonlocal count
        
        product_category = info.get("ì¹´í…Œê³ ë¦¬") or "ê¸°íƒ€"
        category_id = ensure_category_4depth(product_category)

        if already_exists(info["ìƒí’ˆëª…"], category_id):
            return False

        price_val = to_price(info.get("íŒë§¤ê°€ê²©") or "")
        department_price = to_price(info.get("ì‹œì¤‘ê°€ê²©") or "")
        
        if price_val and price_val > MAX_DB_PRICE:
            price_val = MAX_DB_PRICE
        if department_price and department_price > MAX_DB_PRICE:
            department_price = MAX_DB_PRICE
            
        description = f"{info.get('URL','')}\n{info.get('ì„¤ëª…ì´ë¯¸ì§€ë“¤','')}".strip()
        image_url = info.get("ëŒ€í‘œì´ë¯¸ì§€") or ""

        try:
            cur.execute(
                """
                INSERT INTO products (name, description, price, department_price, category_id, image_url, stock, is_active)
                VALUES (%s, %s, %s, %s, %s, %s, %s, true)
                RETURNING id
                """,
                (info["ìƒí’ˆëª…"], description, price_val, 
                 department_price if department_price > 0 else None,
                 category_id, image_url, 10),
            )
            product_id = cur.fetchone()["id"]
        except Exception as exc:
            print(f"[ERROR] DB ì €ì¥ ì˜¤ë¥˜: {exc}")
            return False
        
        options = info.get("ì˜µì…˜", [])
        option_count = save_product_options(product_id, options) if options else 0
        
        count += 1
        sale = info.get("íŒë§¤ê°€ê²©") or "ê°€ê²© ì—†ìŒ"
        opt_info = f", ì˜µì…˜ {option_count}ê°œ" if option_count else ""
        cat_short = (info.get("ì¹´í…Œê³ ë¦¬") or "")[:20]
        print(f"  [+{count}] {info['ìƒí’ˆëª…'][:35]} | {sale} | {cat_short}{opt_info}")
        
        # ì €ì¥ ì„±ê³µ ì‹œ it_id ìºì‹œì— ì¶”ê°€ (ê°™ì€ ì„¸ì…˜ ì¤‘ë³µ ë°©ì§€)
        import re as _re
        m = _re.search(r"it_id=(\d+)", info.get("URL", ""))
        if m:
            existing_it_ids.add(m.group(1))
        
        return True

    # ============================================
    # ë³‘ë ¬ ì²˜ë¦¬ (ì‚¬ì´íŠ¸ë§µ ì¦‰ì‹œ ì²˜ë¦¬ + ì¹´í…Œê³ ë¦¬ ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘)
    # ============================================
    if SKIP_S3_UPLOAD:
        print(f"[S3] S3 ì—…ë¡œë“œ ìŠ¤í‚µ ëª¨ë“œ - ì›ë³¸ ì´ë¯¸ì§€ URLì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print(f"[SCAN] ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (ì›Œì»¤ {MAX_WORKERS}ê°œ, ë°°ì¹˜ {BATCH_SIZE}ê°œ)...")
    print(f"[SCAN] ì‚¬ì´íŠ¸ë§µ URL ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘, ì¹´í…Œê³ ë¦¬ URLì€ ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ì¤‘...")
    
    start_time = time.time()
    batch_idx = 0
    url_index = 0  # í˜„ì¬ ì²˜ë¦¬ ìœ„ì¹˜
    
    while True:
        # ì¤‘ì§€ ìš”ì²­ í™•ì¸
        if check_stop_flag():
            print(f"[STOP] ì¤‘ì§€ë¨ - {count}ê°œ ì €ì¥ ì™„ë£Œ")
            break
        if count >= MAX_SAVE:
            print(f"[DONE] ëª©í‘œ {MAX_SAVE}ê°œ ë‹¬ì„±!")
            break
        
        # ì¹´í…Œê³ ë¦¬ íì—ì„œ ìƒˆ URL ê°€ì ¸ì˜¤ê¸°
        new_urls_added = 0
        while category_url_queue:
            try:
                new_url = category_url_queue.popleft()
                urls.append(new_url)
                new_urls_added += 1
            except IndexError:
                break
        if new_urls_added > 0 and new_urls_added >= 100:
            print(f"[QUEUE] ì¹´í…Œê³ ë¦¬ì—ì„œ {new_urls_added}ê°œ URL ì¶”ê°€ (ì´ {len(urls)}ê°œ)")
        
        # ì²˜ë¦¬í•  URLì´ ì—†ê³ , ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ë„ ëë‚¬ìœ¼ë©´ ì¢…ë£Œ
        if url_index >= len(urls):
            if category_collect_done.is_set():
                # ë§ˆì§€ë§‰ìœ¼ë¡œ í ë¹„ìš°ê¸°
                while category_url_queue:
                    try:
                        urls.append(category_url_queue.popleft())
                    except IndexError:
                        break
                if url_index >= len(urls):
                    print(f"[DONE] ëª¨ë“  URL ì²˜ë¦¬ ì™„ë£Œ!")
                    break
            else:
                # ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘ì´ë©´ ì ì‹œ ëŒ€ê¸°
                time.sleep(1)
                continue
        
        # í˜„ì¬ ë°°ì¹˜ ì¶”ì¶œ
        batch = urls[url_index:url_index + BATCH_SIZE]
        if not batch:
            time.sleep(0.5)
            continue
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(fetch_and_filter, (scanned + i + 1, url)): url 
                for i, url in enumerate(batch)
            }
            
            for future in as_completed(futures):
                if check_stop_flag():
                    executor.shutdown(wait=False, cancel_futures=True)
                    break
                
                info, idx, url, error = future.result()
                scanned += 1
                
                if info:
                    save_product_to_db(info)
                    if count >= MAX_SAVE:
                        break
                elif error and ("timed out" in str(error).lower() or "max retries" in str(error).lower()):
                    retry_urls.append(url)
        
        url_index += len(batch)
        batch_idx += 1
        
        # ì§„í–‰ë¥  í‘œì‹œ (5ë°°ì¹˜ë§ˆë‹¤ = ë” ìì£¼)
        if batch_idx % 5 == 0:
            elapsed = time.time() - start_time
            rate = scanned / elapsed if elapsed > 0 else 0
            save_rate = count / elapsed if elapsed > 0 else 0
            total_known = len(urls)
            cat_status = "ìˆ˜ì§‘ ì¤‘" if not category_collect_done.is_set() else "ì™„ë£Œ"
            remaining_urls = total_known - scanned
            remaining_sec = remaining_urls / rate if rate > 0 else 0
            
            # ì‹œê°„ í¬ë§· (ì‹œê°„/ë¶„/ì´ˆ)
            elapsed_m, elapsed_s = divmod(int(elapsed), 60)
            elapsed_h, elapsed_m = divmod(elapsed_m, 60)
            remain_m, remain_s = divmod(int(remaining_sec), 60)
            remain_h, remain_m = divmod(remain_m, 60)
            
            elapsed_str = f"{elapsed_h}ì‹œê°„ {elapsed_m}ë¶„" if elapsed_h > 0 else f"{elapsed_m}ë¶„ {elapsed_s}ì´ˆ"
            remain_str = f"{remain_h}ì‹œê°„ {remain_m}ë¶„" if remain_h > 0 else f"{remain_m}ë¶„ {remain_s}ì´ˆ"
            
            pct = scanned / total_known * 100 if total_known > 0 else 0
            print(f"[ğŸ“Š ì§„í–‰] {scanned:,}/{total_known:,} ìŠ¤ìº” ({pct:.1f}%) | "
                  f"ì €ì¥: {count:,}ê°œ ({save_rate:.1f}ê°œ/ì´ˆ) | "
                  f"ê²½ê³¼: {elapsed_str} | ë‚¨ì€ì‹œê°„: {remain_str} | "
                  f"ì¹´í…Œê³ ë¦¬URL: {cat_status}")
            
            if batch_idx % 20 == 0:
                gc.collect()
        
        time.sleep(SLEEP_BETWEEN_BATCH)
    
    # ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
    cat_thread.join(timeout=5)
    
    # ============================================
    # ì‹¤íŒ¨í•œ URL ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
    # ============================================
    if retry_urls and not check_stop_flag() and count < MAX_SAVE:
        print(f"\n[RETRY] íƒ€ì„ì•„ì›ƒ/ì—ëŸ¬ {len(retry_urls)}ê°œ URL ì¬ì‹œë„ ì‹œì‘...")
        retry_round = 0
        while retry_urls and retry_round < 2 and not check_stop_flag() and count < MAX_SAVE:
            retry_round += 1
            current_retry = list(retry_urls)
            retry_urls.clear()
            print(f"[RETRY] {retry_round}ì°¨ ì¬ì‹œë„: {len(current_retry)}ê°œ")
            
            for i in range(0, len(current_retry), BATCH_SIZE):
                if check_stop_flag() or count >= MAX_SAVE:
                    break
                batch = current_retry[i:i+BATCH_SIZE]
                
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    futures = {
                        executor.submit(fetch_and_filter, (0, url)): url
                        for url in batch
                    }
                    for future in as_completed(futures):
                        info, idx, url, error = future.result()
                        if info:
                            save_product_to_db(info)
                        elif error and ("timed out" in str(error).lower() or "max retries" in str(error).lower()):
                            retry_urls.append(url)
                
                time.sleep(SLEEP_BETWEEN_BATCH * 2)  # ì¬ì‹œë„ëŠ” ì¢€ ë” ì—¬ìœ  ìˆê²Œ
            
            print(f"[RETRY] {retry_round}ì°¨ ì™„ë£Œ (ë‚¨ì€ ì‹¤íŒ¨: {len(retry_urls)}ê°œ)")
        
        if retry_urls:
            print(f"[RETRY] ìµœì¢… ì‹¤íŒ¨: {len(retry_urls)}ê°œ (ì‚­ì œ/ë¹„ê³µê°œ ìƒí’ˆì¼ ê°€ëŠ¥ì„±)")
    
    elapsed_total = time.time() - start_time
    et_m, et_s = divmod(int(elapsed_total), 60)
    et_h, et_m = divmod(et_m, 60)
    time_str = f"{et_h}ì‹œê°„ {et_m}ë¶„ {et_s}ì´ˆ" if et_h > 0 else f"{et_m}ë¶„ {et_s}ì´ˆ"
    avg_speed = f"{scanned/elapsed_total:.1f}ê°œ/ì´ˆ" if elapsed_total > 0 else "N/A"
    save_speed = f"{count/elapsed_total:.1f}ê°œ/ì´ˆ" if elapsed_total > 0 else "N/A"
    print(f"\n{'='*50}")
    print(f"  í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"  ì´ ìŠ¤ìº”: {scanned:,}ê°œ | ì €ì¥: {count:,}ê°œ")
    print(f"  ì´ URL: {len(urls):,}ê°œ")
    print(f"  ì†Œìš” ì‹œê°„: {time_str}")
    print(f"  ìŠ¤ìº” ì†ë„: {avg_speed} | ì €ì¥ ì†ë„: {save_speed}")
    print(f"{'='*50}")
    
    cur.close()
    conn.close()


def save_to_csv(products: List[Dict], filename: str = CSV_FILENAME) -> None:
    """í¬ë¡¤ë§í•œ ìƒí’ˆ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not products:
        print("ì €ì¥í•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    fieldnames = ["ìƒí’ˆëª…", "ì¹´í…Œê³ ë¦¬", "ì‹œì¤‘ê°€ê²©", "íŒë§¤ê°€ê²©", "ëŒ€í‘œì´ë¯¸ì§€", "ì„¤ëª…ì´ë¯¸ì§€ë“¤", "URL", "ì˜µì…˜"]
    
    with open(filename, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for product in products:
            options_str = json.dumps(product.get("ì˜µì…˜", []), ensure_ascii=False) if product.get("ì˜µì…˜") else ""
            
            writer.writerow({
                "ìƒí’ˆëª…": product.get("ìƒí’ˆëª…", ""),
                "ì¹´í…Œê³ ë¦¬": product.get("ì¹´í…Œê³ ë¦¬", ""),
                "ì‹œì¤‘ê°€ê²©": product.get("ì‹œì¤‘ê°€ê²©", ""),
                "íŒë§¤ê°€ê²©": product.get("íŒë§¤ê°€ê²©", ""),
                "ëŒ€í‘œì´ë¯¸ì§€": product.get("ëŒ€í‘œì´ë¯¸ì§€", ""),
                "ì„¤ëª…ì´ë¯¸ì§€ë“¤": product.get("ì„¤ëª…ì´ë¯¸ì§€ë“¤", ""),
                "URL": product.get("URL", ""),
                "ì˜µì…˜": options_str,
            })
    
    print(f"[OK] CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename} ({len(products)}ê°œ ìƒí’ˆ)")


def crawl_only() -> None:
    """DB ì €ì¥ ì—†ì´ í¬ë¡¤ë§ë§Œ ìˆ˜í–‰í•˜ê³  CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    urls = get_product_urls()
    if not urls:
        print("ìƒí’ˆ URLì„ ì°¾ì§€ ëª»í•´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print(f"í¬ë¡¤ë§ ì‹œì‘ (ì´ {len(urls)}ê°œ í›„ë³´)...")
    
    products = []
    for idx, url in enumerate(urls, start=1):
        if len(products) >= MAX_SAVE:
            print(f"[STOP] ìµœëŒ€ {MAX_SAVE}ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§ í›„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
        
        print(f"[{idx}/{len(urls)}] ìˆ˜ì§‘ ì¤‘: {url}")
        info = parse_product_detail(url)
        if info:
            products.append(info)
            options = info.get("ì˜µì…˜", [])
            opt_info = f", ì˜µì…˜ {sum(len(o.get('values', [])) for o in options)}ê°œ" if options else ""
            print(f"  [OK] ìˆ˜ì§‘: {info['ìƒí’ˆëª…']}{opt_info}")
        
        time.sleep(random.uniform(1, 3))
    
    save_to_csv(products)
    print(f"ì™„ë£Œ! ì´ {len(products)}ê°œì˜ ìƒí’ˆì„ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--csv-only":
        crawl_only()
    else:
        main()
